{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is to test the sentiment_mod.py \n",
    "# File which imports all the necessary data for the sentiment analysis\n",
    "# through loaded pickles, classes and functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Generic NLTK library\n",
    "# sentence, work tokenizer and unsupervised sentence tokenizer which can be trained and implemented\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer \n",
    "# Stemming tool \n",
    "from nltk.stem import PorterStemmer\n",
    "# Text of all the state of union speeches\n",
    "from nltk.corpus import state_union\n",
    "# Text of all stop words\n",
    "from nltk.corpus import stopwords\n",
    "# Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Frequence distribution\n",
    "from nltk import FreqDist\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "import random \n",
    "import io\n",
    "import pickle\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a class to build an ensemble classifier\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self,*classifiers): # default method\n",
    "        self._classifiers=classifiers\n",
    "        \n",
    "    def classify(self, features): # returns mode of votes\n",
    "        votes=[]\n",
    "        for c in self._classifiers:\n",
    "            v=c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features): # returns confidence: fraction of positive votes \n",
    "        votes=[]\n",
    "        for c in self._classifiers:\n",
    "            v=c.classify(features)\n",
    "            votes.append(v)\n",
    "            \n",
    "        choice_votes=votes.count(mode(votes))\n",
    "        conf =choice_votes/len(votes)\n",
    "        \n",
    "        return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loading Documents from pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back up code - in case if the pickle is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Using an example of POS tagging based sentiment analysis\n",
    "\n",
    "# import io # Harrison's code did not word to read\n",
    "# # Got this format of importing text from stackoverflow\n",
    "# short_pos=io.open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/positive.txt\",encoding='latin-1').read()\n",
    "# short_neg=io.open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/negative.txt\",encoding='latin-1').read()\n",
    "\n",
    "# documents=[]\n",
    "# for r in short_pos.split('\\n'): # splitting document by new line\n",
    "#     documents.append((r,\"pos\")) # tuple- text,sentiment appended\n",
    "# for r in short_neg.split('\\n'):\n",
    "#     documents.append((r,\"neg\"))\n",
    "\n",
    "\n",
    "# # j is adjective, r is adverb and v is verb\n",
    "# # allowed word types=[\"J\",\"R\",\"V\"]\n",
    "# allowed_word_types=[\"J\"] # Only allowing adjectives\n",
    "# all_words=[]\n",
    "# for p in short_pos.split('\\n'): # extracting all separate lines from positive tagged documents\n",
    "#     documents.append((p,\"pos\")) # appending all lines from positive tagged documents\n",
    "#     words=word_tokenize(p) # word tokenizing from the lines\n",
    "#     pos=nltk.pos_tag(words) # getting the POS of the words\n",
    "    \n",
    "#     for w in pos: # w[1][0] gives the first letter of POS of the word\n",
    "#         if w[1][0] in allowed_word_types: # checking for adjectives\n",
    "#             all_words.append(w[0].lower())# w[0] gives the word and POS\n",
    " \n",
    "\n",
    "# for p in short_neg.split('\\n'): # extracting all separate lines from negative tagged documents\n",
    "#     documents.append((p,\"neg\")) # appending all lines from positive negative documents\n",
    "#     words=word_tokenize(p) # word tokenizing from the lines\n",
    "#     pos=nltk.pos_tag(words) # getting the POS of the words\n",
    "    \n",
    "#     for w in pos: # w[1][0] gives the first letter of POS of the word\n",
    "#         if w[1][0] in allowed_word_types: # checking for adjectives\n",
    "#             all_words.append(w[0].lower())# w[0] gives the word and POS\n",
    "\n",
    "# # saving documents to pickles\n",
    "\n",
    "# save_documents=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/documents.pickle\",\"wb\")\n",
    "# pickle.dump(documents,save_documents)\n",
    "# save_documents.close()\n",
    "\n",
    "# all_words=nltk.FreqDist(all_words)\n",
    "# word_features=list(all_words.keys())[:5000] # get top 5000 frequent words \n",
    "\n",
    "# save_word_features=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/word_features5k.pickle\",\"wb\")\n",
    "# pickle.dump(word_features,save_word_features)\n",
    "# save_word_features.close()\n",
    "\n",
    "# # check if the features are in top words\n",
    "# def find_features(document):\n",
    "#     words=word_tokenize(document)\n",
    "#     features={}\n",
    "#     for w in word_features:\n",
    "#         features[w]=(w in words)\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# # building featuresets\n",
    "# featuresets=[(find_features(rev),category) for (rev,category) in documents]\n",
    "\n",
    "# random.shuffle(featuresets) #shuffling feature sets for train test\n",
    "\n",
    "# save_featuresets= open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/featuresets.pickle\",\"wb\")\n",
    "# pickle.dump(featuresets,save_featuresets)\n",
    "# save_featuresets.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_f=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/documents.pickle\",\"rb\")\n",
    "documents=pickle.load(documents_f)\n",
    "documents_f.close()\n",
    "\n",
    "word_features5k_f= open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/word_features5k.pickle\",\"rb\")\n",
    "word_features=pickle.load(word_features5k_f)\n",
    "word_features5k_f.close()\n",
    "\n",
    "featuresets_f= open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/featuresets.pickle\",\"rb\")\n",
    "featuresets=pickle.load(featuresets_f)\n",
    "featuresets_f.close()\n",
    "\n",
    "# def find_features(document):\n",
    "#     words=word_tokenize(document)\n",
    "#     features={}\n",
    "#     for w in word_features:\n",
    "#         features[w]=(w in words)\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# # building featuresets\n",
    "# featuresets=[(find_features(rev),category) for (rev,category) in documents]\n",
    "\n",
    "# random.shuffle(featuresets) #shuffling feature sets for train test\n",
    "\n",
    "training_set=featuresets[:10000]\n",
    "testing_set=featuresets[10000:]\n",
    "\n",
    "open_file=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/MNB_classifier5k.pickle\",\"rb\")\n",
    "MNB_clasifier=pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/BernoulliNB_classifier5k.pickle\",\"rb\")\n",
    "BernoulliNB_clasifier=pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/LogisticRegression_classifier5k.pickle\",\"rb\")\n",
    "LogisticRegression_clasifier=pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/SGDClassifier_classifier5k.pickle\",\"rb\")\n",
    "SGDClassifier_clasifier=pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/SVC_classifier5k.pickle\",\"rb\")\n",
    "SVC_clasifier=pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/LinearSVC_classifier5k.pickle\",\"rb\")\n",
    "LinearSVC_clasifier=pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "# open_file=open(\"/Users/vputcha/Documents_Venkat/Kaggle/NLTK/pickled_algos/NuSVC_classifier5k.pickle\",\"rb\")\n",
    "# NuSVC_classifier=pickle.lead(open_file)\n",
    "# open_file.close()\n",
    "\n",
    "\n",
    "voted_classifier = VoteClassifier(\n",
    "                                  MNB_clasifier,\n",
    "                                  BernoulliNB_clasifier,\n",
    "                                  LogisticRegression_clasifier,\n",
    "                                  SGDClassifier_clasifier,\n",
    "                                  SVC_clasifier,\n",
    "                                  LinearSVC_clasifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment of text and confidence of prediction\n",
    "def sentiment(text):\n",
    "    feats=find_features(text)\n",
    "    \n",
    "    return voted_classifer.classify(feats),voted_classifier.confidence(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
